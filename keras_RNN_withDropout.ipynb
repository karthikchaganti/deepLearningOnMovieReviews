{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CSE 701 Seminar Project\n",
    "# TITLE : Sentiment Analysis of Movie Reviews using LSTM and RNN\n",
    "# Author: Karthik Chaganti\n",
    "# UBID: 50169441\n",
    "# Referenceo that for instance the integer \": Jason Brownlee (machinelearningmastery.com)\n",
    "\n",
    "# Libraries : Keras on Theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The IMDB dataset consists of 25,000 reviews. Each review is represented as sequences(lists) of word indexes. Each\n",
    "# index represents the overall frequency of the word in the whole dataset. Eg: lets say 'good' is the one word that\n",
    "# appears the most in the dataset, so its assigned an index 1. So in a review, whereever good appears, 1 is used to\n",
    "# represen that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.gof.compilelock): Overriding existing lock by dead process '4546' (I am process '4889')\n",
      "Using gpu device 0: GeForce GTX 960M (CNMeM is enabled with initial size: 80.0% of memory, cuDNN 5005)\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "# import imdb dataset\n",
    "from keras.datasets import imdb \n",
    "# import the sequential, dense for representation of model in keras\n",
    "# models in keras are defined as a sequence of layers hence sequential\n",
    "from keras.models import Sequential\n",
    "# fully connected layers in keras are represented using Dense class\n",
    "from keras.layers import Dense\n",
    "# import the RNN library\n",
    "from keras.layers import SimpleRNN\n",
    "# embedding in keras converts positive integer representation of words\n",
    "# into a word embedding encoded as real-valued vectors in space. Here\n",
    "# the similarity between words in terms of meaning translates to \n",
    "# proximity in vector space\n",
    "from keras.layers.embeddings import Embedding\n",
    "# sequences are list of lists of datatypes\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "# load the dataset. \n",
    "# Keep the top k highest frequency terms for ease of use. \n",
    "top_k = 10000 \n",
    "(X_train, y_train),(X_test,y_test) = imdb.load_data(nb_words = top_k)\n",
    "#print(X_train)\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# truncate the excessive length reviews to a length of 600 words\n",
    "max_rev_length = 600\n",
    "X_train = sequence.pad_sequences(X_train, maxlen = max_rev_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen = max_rev_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_1 (Embedding)          (None, 600, 32)       320000      embedding_input_1[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "simplernn_1 (SimpleRNN)          (None, 100)           13300       embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 1)             101         simplernn_1[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 333401\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 31s - loss: 0.7163 - acc: 0.4978    \n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 32s - loss: 0.7007 - acc: 0.4993    \n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 31s - loss: 0.6967 - acc: 0.5036    \n",
      "Accuracy:  50.92%\n"
     ]
    }
   ],
   "source": [
    "# Design the model\n",
    "# Using embedding layer, we can convert the number representation of words\n",
    "# to real-valued vector space a.k.a. word embedding\n",
    "\n",
    "# Create an embed vector of length 32\n",
    "embed_length = 32\n",
    "# Use the sequential Model. For bigger modesls, functional api can be used\n",
    "model = Sequential()\n",
    "# Add the embedding layer to represent the words \n",
    "model.add(Embedding(top_k,embed_length,input_length = max_rev_length))\n",
    "# add the RNN layer containing 100 mem units with dropout of 0.2\n",
    "model.add(SimpleRNN(100, dropout_W=0.2,dropout_U=0.2))\n",
    "# Dense layer connects each input neuron to the output neurons.\n",
    "# It deals with density of connections of neurons\n",
    "# Here we are using connections on each neuron to every other output neuron\n",
    "# Activation Function: Sigmoid\n",
    "model.add(Dense(1,activation = 'sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "# train the model with no.of. epochs\n",
    "model.fit(X_train,y_train,nb_epoch = 3, batch_size=64)\n",
    "# Evaluate Model\n",
    "scores = model.evaluate(X_test,y_test,verbose = 0)\n",
    "print(\"Accuracy:  %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
